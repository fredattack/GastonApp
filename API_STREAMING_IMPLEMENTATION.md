# API Streaming Implementation Guide

## Vue d'ensemble

Ce document d√©crit l'impl√©mentation n√©cessaire c√¥t√© backend (Laravel) pour supporter le streaming des r√©ponses AI dans l'interface conversationnelle.

## √âtat Actuel

### ‚úÖ Ce qui fonctionne d√©j√†

**Endpoint existant : `POST /api/v1-0-0/ai`**

```json
Request:
{
  "prompt": "Pablo doit manger 365gr de barf demain matin",
  "filters": {}
}

Response:
{
  "score": 95,
  "requestType": "createEvent",
  "description": "Cr√©ation d'un √©v√©nement de nourriture pour Pablo demain matin",
  "data": {
    "title": "Repas de Pablo - Barf",
    "type": "feeding",
    "petId": [3],
    "start_date": "2025-11-12 08:00:00",
    "end_date": null,
    "is_recurring": false,
    "is_full_day": false,
    "pets": [
      {
        "id": 3,
        "pivot": {
          "item": "Barf",
          "quantity": "365gr",
          "notes": ""
        }
      }
    ],
    "notes": "Repas du matin"
  }
}
```

‚úÖ **Fallback automatique** : Le frontend utilise cet endpoint et simule le streaming visuellement.

---

## üöÄ Impl√©mentation du Streaming (Optionnel mais Recommand√©)

### Nouvel endpoint : `POST /api/v1-0-0/ai/stream`

#### 1. Route Laravel

```php
// routes/api.php
Route::post('/v1-0-0/ai/stream', [AIController::class, 'stream'])->name('ai.stream');
```

#### 2. Controller

```php
<?php

namespace App\Http\Controllers\API;

use Illuminate\Http\Request;
use Illuminate\Support\Facades\Http;
use App\Http\Controllers\Controller;

class AIController extends Controller
{
    /**
     * Stream AI response avec Server-Sent Events
     *
     * @param Request $request
     * @return \Symfony\Component\HttpFoundation\StreamedResponse
     */
    public function stream(Request $request)
    {
        $messages = $request->input('messages', []);

        // Validation
        if (empty($messages)) {
            return response()->json(['error' => 'Messages required'], 400);
        }

        return response()->stream(
            function () use ($messages) {
                // Configuration OpenAI
                $apiKey = config('services.openai.key');
                $model = config('services.openai.model', 'gpt-4');

                // Pr√©parer le contexte syst√®me
                $systemMessage = [
                    'role' => 'system',
                    'content' => $this->getSystemPrompt()
                ];

                // Combiner avec les messages utilisateur
                $allMessages = array_merge([$systemMessage], $messages);

                try {
                    // Appel √† OpenAI avec streaming
                    $response = Http::withHeaders([
                        'Authorization' => 'Bearer ' . $apiKey,
                        'Content-Type' => 'application/json',
                    ])
                    ->timeout(60)
                    ->withOptions(['stream' => true])
                    ->post('https://api.openai.com/v1/chat/completions', [
                        'model' => $model,
                        'messages' => $allMessages,
                        'stream' => true,
                        'temperature' => 0.7,
                    ]);

                    // Stream les chunks au client
                    $fullContent = '';

                    foreach ($response->body() as $chunk) {
                        $lines = explode("\n", $chunk);

                        foreach ($lines as $line) {
                            if (strpos($line, 'data: ') === 0) {
                                $data = substr($line, 6);

                                if ($data === '[DONE]') {
                                    continue;
                                }

                                try {
                                    $decoded = json_decode($data, true);

                                    if (isset($decoded['choices'][0]['delta']['content'])) {
                                        $content = $decoded['choices'][0]['delta']['content'];
                                        $fullContent .= $content;

                                        // Envoyer le chunk au client
                                        echo "data: " . json_encode([
                                            'chunk' => $content,
                                            'done' => false
                                        ]) . "\n\n";

                                        ob_flush();
                                        flush();
                                    }
                                } catch (\Exception $e) {
                                    \Log::error('Stream parsing error: ' . $e->getMessage());
                                }
                            }
                        }
                    }

                    // Analyser le contenu complet pour g√©n√©rer la r√©ponse structur√©e
                    $finalResponse = $this->parseAIResponse($fullContent);

                    // Envoyer la r√©ponse finale
                    echo "data: " . json_encode([
                        'chunk' => '',
                        'done' => true,
                        'response' => $finalResponse
                    ]) . "\n\n";

                    ob_flush();
                    flush();

                } catch (\Exception $e) {
                    \Log::error('AI Streaming error: ' . $e->getMessage());

                    echo "data: " . json_encode([
                        'error' => $e->getMessage(),
                        'done' => true
                    ]) . "\n\n";

                    ob_flush();
                    flush();
                }
            },
            200,
            [
                'Content-Type' => 'text/event-stream',
                'Cache-Control' => 'no-cache',
                'X-Accel-Buffering' => 'no',
                'Connection' => 'keep-alive',
            ]
        );
    }

    /**
     * Parse la r√©ponse AI pour extraire les donn√©es structur√©es
     */
    private function parseAIResponse(string $content): array
    {
        // R√©utiliser la logique existante de l'endpoint /ai
        // Pour parser le contenu et extraire les donn√©es structur√©es

        // Exemple simplifi√©
        return [
            'score' => 95,
            'requestType' => 'createEvent',
            'description' => $content,
            'data' => $this->extractEventData($content)
        ];
    }

    /**
     * Extraire les donn√©es d'√©v√©nement du contenu
     */
    private function extractEventData(string $content): array
    {
        // Logique pour extraire les donn√©es structur√©es
        // √Ä adapter selon votre impl√©mentation actuelle

        return [
            'title' => '',
            'type' => 'feeding',
            'petId' => [],
            'start_date' => now()->addDay()->format('Y-m-d H:i:s'),
            'end_date' => null,
            'is_recurring' => false,
            'is_full_day' => false,
            'pets' => [],
            'notes' => ''
        ];
    }

    /**
     * Prompt syst√®me pour l'IA
     */
    private function getSystemPrompt(): string
    {
        return "Tu es un assistant pour la gestion d'animaux de compagnie.
        Tu aides √† cr√©er des √©v√©nements (repas, soins v√©t√©rinaires, etc.)
        en analysant les demandes en langage naturel.

        R√©ponds de mani√®re naturelle et conversationnelle.

        Les types d'√©v√©nements possibles sont :
        - feeding (alimentation)
        - medical (soins m√©dicaux)
        - appointment (rendez-vous)
        - training (dressage)
        - social (socialisation)";
    }
}
```

#### 3. Configuration

```php
// config/services.php
return [
    // ...
    'openai' => [
        'key' => env('OPENAI_API_KEY'),
        'model' => env('OPENAI_MODEL', 'gpt-4'),
    ],
];
```

```env
# .env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4
```

---

## üîÑ Alternative : Am√©liorer l'endpoint existant

Si tu ne veux pas impl√©menter le streaming imm√©diatement, tu peux am√©liorer l'endpoint existant pour supporter le contexte multi-tour :

```php
/**
 * POST /api/v1-0-0/ai
 * Support pour contexte conversationnel
 */
public function analyze(Request $request)
{
    $validated = $request->validate([
        'prompt' => 'required_without:messages|string',
        'messages' => 'required_without:prompt|array',
        'messages.*.role' => 'required_with:messages|in:user,assistant,system',
        'messages.*.content' => 'required_with:messages|string',
        'filters' => 'array',
    ]);

    // Si messages (nouveau format multi-tour)
    if (isset($validated['messages'])) {
        $messages = $validated['messages'];
        $lastUserMessage = collect($messages)
            ->reverse()
            ->firstWhere('role', 'user');

        $prompt = $lastUserMessage['content'] ?? '';
    } else {
        // Format legacy (simple prompt)
        $prompt = $validated['prompt'];
        $messages = [
            ['role' => 'user', 'content' => $prompt]
        ];
    }

    // Votre logique existante...
    $response = $this->callOpenAI($messages);

    return response()->json($response);
}
```

---

## üìä Comparaison des Options

### Option 1 : Garder le syst√®me actuel (Fallback uniquement)

**Avantages** :
- ‚úÖ Aucune modification n√©cessaire
- ‚úÖ Fonctionne d√©j√† bien
- ‚úÖ Simulation de streaming c√¥t√© frontend

**Inconv√©nients** :
- ‚ùå Pas de vrai streaming (attente compl√®te de la r√©ponse)
- ‚ùå Latence per√ßue plus √©lev√©e
- ‚ùå Pas de contexte multi-tour

### Option 2 : Endpoint am√©lior√© (sans streaming)

**Avantages** :
- ‚úÖ Support du contexte multi-tour
- ‚úÖ Conversations plus intelligentes
- ‚úÖ Modification minimale

**Inconv√©nients** :
- ‚ùå Pas de vrai streaming
- ‚ùå Attente compl√®te de la r√©ponse

### Option 3 : Streaming complet (Recommand√©)

**Avantages** :
- ‚úÖ Exp√©rience utilisateur optimale
- ‚úÖ Feedback en temps r√©el
- ‚úÖ Contexte multi-tour
- ‚úÖ Perception de rapidit√©

**Inconv√©nients** :
- ‚ö†Ô∏è Impl√©mentation plus complexe
- ‚ö†Ô∏è N√©cessite des tests

---

## üß™ Tests

### Test manuel du streaming

```bash
# Test avec curl
curl -X POST http://localhost:3008/api/v1-0-0/ai/stream \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "Pablo doit manger 365gr de barf demain matin"
      }
    ]
  }'
```

### Test avec le frontend

1. Ouvrir l'application : `http://localhost:4481/ai-assistant`
2. Envoyer un message
3. Observer dans la console :
   - ‚úÖ Si streaming OK : Pas de warning
   - ‚ö†Ô∏è Si fallback : "Streaming endpoint not available, falling back to regular API"

---

## üìù Recommandations

### Court terme (Option 2)
1. Modifier l'endpoint `/ai` pour supporter `messages[]`
2. Garder le fallback actuel
3. Am√©liorer le contexte conversationnel

### Long terme (Option 3)
1. Impl√©menter `/ai/stream`
2. Tester avec des conversations r√©elles
3. Monitorer les performances
4. Optimiser si n√©cessaire

---

## üîê S√©curit√©

```php
// Middleware pour limiter le rate limiting
Route::middleware(['throttle:ai'])->group(function () {
    Route::post('/ai', [AIController::class, 'analyze']);
    Route::post('/ai/stream', [AIController::class, 'stream']);
});

// config/throttle.php
'ai' => [
    'max_attempts' => 20, // 20 requ√™tes
    'decay_minutes' => 1, // par minute
],
```

---

## üí∞ Consid√©rations de Co√ªt

- Streaming = M√™me co√ªt qu'une requ√™te normale
- Avantage : Meilleure UX sans surco√ªt
- Monitorer l'utilisation de tokens OpenAI

---

## üêõ Debugging

```php
// Log des requ√™tes AI
\Log::channel('ai')->info('AI Request', [
    'messages' => $messages,
    'user_id' => auth()->id(),
]);

// Log des r√©ponses
\Log::channel('ai')->info('AI Response', [
    'response' => $response,
    'duration' => $duration,
]);
```

---

## ‚úÖ Checklist de D√©ploiement

- [ ] Endpoint `/ai/stream` impl√©ment√©
- [ ] Tests unitaires √©crits
- [ ] Tests d'int√©gration passants
- [ ] Rate limiting configur√©
- [ ] Logging activ√©
- [ ] Variables d'environnement configur√©es
- [ ] Documentation √† jour
- [ ] Frontend test√© avec vrai streaming
- [ ] Monitoring activ√©

---

## üìû Support

En cas de probl√®me :
1. V√©rifier les logs Laravel : `storage/logs/laravel.log`
2. V√©rifier la console browser : Network tab
3. Tester avec curl directement
4. V√©rifier la cl√© OpenAI

